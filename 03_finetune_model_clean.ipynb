{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 03 · Fine-Tune Nanochat Science QA Model (Clean Version)\n",
        "\n",
        "This notebook configures and launches supervised fine-tuning for the Nanochat model on the ScienceQA conversational dataset prepared in the earlier steps.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prerequisites\n",
        "\n",
        "- Run `nanochat-QA_finetune.ipynb` to generate the `data/*_formatted` artifacts.\n",
        "- Run `02_load_base_model.ipynb` to cache the base `sdobson/nanochat` weights.\n",
        "- Ensure the required Python packages are installed.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cleanup Summary\n",
        "\n",
        "- Removed the standalone GPU status check cell from the earlier draft because GPU verification already occurs in Notebook 2 (`02_load_base_model.ipynb`) and the fine-tuning flow here relies on `device_map=\"auto\"`.\n",
        "- No other structural changes were required; remaining cells continue the Task 3 logic for configuring training, running fine-tuning, and saving checkpoints.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Removed GPU status check; see Cleanup Summary above for rationale."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import json\n",
        "import torch\n",
        "\n",
        "project_dir = Path.cwd()\n",
        "data_dir = project_dir / \"data\"\n",
        "output_dir = project_dir / \"nanochat-science-finetuned\"\n",
        "final_dir = project_dir / \"nanochat-science-final\"\n",
        "\n",
        "print(f\"Project directory: {project_dir}\")\n",
        "print(f\"Data directory: {data_dir}\")\n",
        "print(f\"Output directory: {output_dir}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(f\"CUDA device detected: {torch.cuda.get_device_name(0)}\")\n",
        "    torch.backends.cuda.matmul.allow_tf32 = True\n",
        "    torch.backends.cudnn.allow_tf32 = True\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"Warning: Training on CPU will be extremely slow.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Prepared Datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import load_from_disk\n",
        "\n",
        "train_dataset_path = data_dir / \"train_formatted\"\n",
        "val_dataset_path = data_dir / \"val_formatted\"\n",
        "\n",
        "if not train_dataset_path.exists():\n",
        "    raise FileNotFoundError(f\"Missing dataset: {train_dataset_path}. Run nanochat-QA_finetune.ipynb first.\")\n",
        "if not val_dataset_path.exists():\n",
        "    raise FileNotFoundError(f\"Missing dataset: {val_dataset_path}. Run nanochat-QA_finetune.ipynb first.\")\n",
        "\n",
        "train_dataset = load_from_disk(str(train_dataset_path))\n",
        "val_dataset = load_from_disk(str(val_dataset_path))\n",
        "\n",
        "print(train_dataset)\n",
        "print(val_dataset)\n",
        "print(f\"Train samples: {len(train_dataset)} | Validation samples: {len(val_dataset)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Base Model and Tokenizer\n",
        "\n",
        "This cell loads the model using the `transformers` library, which is required for compatibility with the `Trainer` API used for fine-tuning.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    DataCollatorForLanguageModeling,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        ")\n",
        "\n",
        "model_name = \"sdobson/nanochat\"\n",
        "\n",
        "print(f\"Loading tokenizer and model: {model_name}\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    tokenizer.padding_side = \"right\"\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "model.config.use_cache = False  # Required when using gradient checkpointing\n",
        "\n",
        "print(\"Model and tokenizer loaded successfully.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tokenize Conversations for Causal LM Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "max_length = 512\n",
        "\n",
        "\n",
        "def conversation_to_text(messages):\n",
        "    \"\"\"Convert a chat-style dict of messages into a single training string.\"\"\"\n",
        "    segments = []\n",
        "    for message in messages:\n",
        "        role = message.get(\"role\", \"user\")\n",
        "        content = message.get(\"content\", \"\").strip()\n",
        "        segments.append(f'{role}: {content}')\n",
        "    return \"\\n\".join(segments)\n",
        "\n",
        "\n",
        "def tokenize_conversation(example):\n",
        "    text = conversation_to_text(example[\"messages\"])\n",
        "    return tokenizer(\n",
        "        text,\n",
        "        truncation=True,\n",
        "        max_length=max_length,\n",
        "        padding=False,\n",
        "        return_attention_mask=True,\n",
        "    )\n",
        "\n",
        "\n",
        "train_tokenized = train_dataset.map(\n",
        "    tokenize_conversation,\n",
        "    remove_columns=train_dataset.column_names,\n",
        "    desc=\"Tokenizing train dataset\",\n",
        ")\n",
        "\n",
        "val_tokenized = val_dataset.map(\n",
        "    tokenize_conversation,\n",
        "    remove_columns=val_dataset.column_names,\n",
        "    desc=\"Tokenizing validation dataset\",\n",
        ")\n",
        "\n",
        "print(train_tokenized)\n",
        "print(val_tokenized)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configure Data Collator\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False,\n",
        ")\n",
        "\n",
        "print(\"Data collator initialized (causal LM mode).\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Set Training Arguments\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "supports_bf16 = False\n",
        "if torch.cuda.is_available():\n",
        "    compute_capability = torch.cuda.get_device_capability()\n",
        "    supports_bf16 = compute_capability[0] >= 8\n",
        "\n",
        "run_name = f\"nanochat-science-ft-{datetime.utcnow().strftime('%Y%m%d-%H%M%S')}\"\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=str(output_dir),\n",
        "    run_name=run_name,\n",
        "    num_train_epochs=2,\n",
        "    per_device_train_batch_size=2,\n",
        "    per_device_eval_batch_size=2,\n",
        "    gradient_accumulation_steps=8,\n",
        "    learning_rate=5e-5,\n",
        "    warmup_steps=100,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=str(output_dir / \"logs\"),\n",
        "    logging_steps=50,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=200,\n",
        "    save_total_limit=2,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=200,\n",
        "    metric_for_best_model=\"loss\",\n",
        "    load_best_model_at_end=True,\n",
        "    fp16=torch.cuda.is_available() and not supports_bf16,\n",
        "    bf16=supports_bf16,\n",
        "    gradient_checkpointing=True,\n",
        "    report_to=\"none\",\n",
        "    dataloader_num_workers=2,\n",
        "    optim=\"adamw_torch\",\n",
        ")\n",
        "\n",
        "print(training_args)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Initialize Trainer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if training_args.gradient_checkpointing:\n",
        "    model.gradient_checkpointing_enable()\n",
        "    model.enable_input_require_grads()  # Required for gradient checkpointing in newer HF versions\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_tokenized,\n",
        "    eval_dataset=val_tokenized,\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "print(\"Trainer initialized.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run Fine-Tuning\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "resume_checkpoint = None  # Set to checkpoint path to resume training if interrupted.\n",
        "\n",
        "print(\"Starting fine-tuning — this may take several hours depending on GPU availability...\")\n",
        "train_result = trainer.train(resume_from_checkpoint=resume_checkpoint)\n",
        "trainer.save_state()\n",
        "\n",
        "trainer.log_metrics(\"train\", train_result.metrics)\n",
        "trainer.save_metrics(\"train\", train_result.metrics)\n",
        "\n",
        "print(\"Training complete.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluate Best Checkpoint\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "eval_metrics = trainer.evaluate()\n",
        "trainer.log_metrics(\"eval\", eval_metrics)\n",
        "trainer.save_metrics(\"eval\", eval_metrics)\n",
        "\n",
        "print(eval_metrics)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save Final Model Artifacts\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "final_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "trainer.save_model(str(final_dir))\n",
        "tokenizer.save_pretrained(str(final_dir))\n",
        "\n",
        "metrics_summary = {\n",
        "    \"train\": train_result.metrics,\n",
        "    \"eval\": eval_metrics if \"eval_metrics\" in locals() else None,\n",
        "}\n",
        "\n",
        "metrics_path = final_dir / \"training_metrics.json\"\n",
        "with metrics_path.open(\"w\") as fp:\n",
        "    json.dump(metrics_summary, fp, indent=2)\n",
        "\n",
        "print(f\"Saved fine-tuned model and tokenizer to {final_dir}\")\n",
        "print(f\"Metrics written to {metrics_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## (Optional) Push to Hugging Face Hub\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from huggingface_hub import HfApi\n",
        "#\n",
        "# repo_id = \"your-username/nanochat-science-qa\"\n",
        "# api = HfApi()\n",
        "# api.create_repo(repo_id=repo_id, exist_ok=True)\n",
        "# trainer.push_to_hub()\n",
        "# tokenizer.push_to_hub(repo_id)\n",
        "\n",
        "print(\"Configure and uncomment the cell above to push the model to the Hugging Face Hub.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Next Steps\n",
        "\n",
        "- Validate the fine-tuned model in `04_evaluation.ipynb`.\n",
        "- Build interactive demos in `05_interactive_demo.ipynb`.\n",
        "- Document cost, runtime, and findings for the final report.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
