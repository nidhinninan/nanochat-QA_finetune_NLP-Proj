{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "089de064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available. Using CUDA.\n",
      "Device Name: Quadro T2000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check if CUDA (NVIDIA GPU) is available and set the device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available. Using CUDA.\")\n",
    "    # You can also check the GPU name\n",
    "    print(f\"Device Name: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU not available, using CPU. This will be very slow.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71bfb0df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nanochat repository already exists.\n",
      "Downloading nanochat model and tokenizer files...\n",
      "model_000650.pt already exists. Skipping download.\n",
      "meta_000650.json already exists. Skipping download.\n",
      "tokenizer.pkl already exists. Skipping download.\n",
      "token_bytes.pt already exists. Skipping download.\n",
      "\n",
      "Loading model using nanochat's native functions...\n",
      "\n",
      "Model loaded successfully!\n",
      "Model is on device: cpu\n",
      "Model parameters: 561M\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tiktoken'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 97\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, tokens):\n\u001b[1;32m     95\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer_model\u001b[38;5;241m.\u001b[39mdecode(tokens)\n\u001b[0;32m---> 97\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mNanoTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCustom tokenizer loaded.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[8], line 86\u001b[0m, in \u001b[0;36mNanoTokenizer.__init__\u001b[0;34m(self, cache_dir)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, cache_dir):\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;66;03m# We need to load the tokenizer from the nanochat package now\u001b[39;00m\n\u001b[0;32m---> 86\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnanochat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenizer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Tokenizer\n\u001b[1;32m     87\u001b[0m     tokenizer_path \u001b[38;5;241m=\u001b[39m cache_dir \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtokenizer\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer_model \u001b[38;5;241m=\u001b[39m Tokenizer(\u001b[38;5;28mstr\u001b[39m(tokenizer_path))\n",
      "File \u001b[0;32m~/Desktop/UCMO Classes/Natural Language Processing/Project/nanochat/nanochat/tokenizer.py:153\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpickle\u001b[39;00m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mrustbpe\u001b[39;00m\n\u001b[0;32m--> 153\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtiktoken\u001b[39;00m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mRustBPETokenizer\u001b[39;00m:\n\u001b[1;32m    156\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Light wrapper around tiktoken (for efficient inference) but train with rustbpe\"\"\"\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tiktoken'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "import pickle\n",
    "import json\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "# --- 1. Set up the nanochat environment ---\n",
    "if not Path('nanochat').exists():\n",
    "    print(\"Cloning karpathy/nanochat repository...\")\n",
    "    subprocess.run(['git', 'clone', 'https://github.com/karpathy/nanochat.git'], check=True)\n",
    "else:\n",
    "    print(\"nanochat repository already exists.\")\n",
    "\n",
    "# Add the correct subfolder to the Python path to import its modules\n",
    "import sys\n",
    "# The actual package is inside the 'nanochat/nanochat' directory\n",
    "package_path = os.path.abspath('nanochat')\n",
    "if package_path not in sys.path:\n",
    "    sys.path.insert(0, package_path)\n",
    "\n",
    "# Import from the correct module within the package\n",
    "from nanochat.gpt import GPT, GPTConfig\n",
    "\n",
    "# --- 2. Manually download model and tokenizer files ---\n",
    "print(\"Downloading nanochat model and tokenizer files...\")\n",
    "model_repo = \"sdobson/nanochat\"\n",
    "cache_dir = Path.home() / '.cache' / 'nanochat'\n",
    "\n",
    "# Define files and their target directories within the cache\n",
    "files_to_download = {\n",
    "    \"model_000650.pt\": \"chatsft_checkpoints/d20/\",\n",
    "    \"meta_000650.json\": \"chatsft_checkpoints/d20/\",\n",
    "    \"tokenizer.pkl\": \"tokenizer/\",\n",
    "    \"token_bytes.pt\": \"tokenizer/\"\n",
    "}\n",
    "\n",
    "# Download all files to their respective cache directories\n",
    "for filename, target_subdir in files_to_download.items():\n",
    "    local_dir = cache_dir / target_subdir\n",
    "    os.makedirs(local_dir, exist_ok=True)\n",
    "    target_path = local_dir / filename\n",
    "    if not target_path.exists():\n",
    "        print(f\"Downloading {filename}...\")\n",
    "        hf_hub_download(repo_id=model_repo, filename=filename, local_dir=local_dir, local_dir_use_symlinks=False)\n",
    "    else:\n",
    "        print(f\"{filename} already exists. Skipping download.\")\n",
    "\n",
    "# --- 3. Load the model using nanochat's own scripts ---\n",
    "print(\"\\nLoading model using nanochat's native functions...\")\n",
    "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device = 'cpu'\n",
    "\n",
    "# Load the model checkpoint\n",
    "checkpoint_path = cache_dir / \"chatsft_checkpoints\" / \"d20\" / \"model_000650.pt\"\n",
    "meta_path = cache_dir / \"chatsft_checkpoints\" / \"d20\" / \"meta_000650.json\"\n",
    "\n",
    "state_dict = torch.load(checkpoint_path, map_location=device)\n",
    "\n",
    "with open(meta_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    meta = json.load(f)\n",
    "\n",
    "gptconf = GPTConfig(**meta['model_config'])\n",
    "model = GPT(gptconf)\n",
    "model.load_state_dict(state_dict, strict=True)\n",
    "\n",
    "# Fix the keys in the state_dict (remove '_orig_mod.' prefix)\n",
    "unwanted_prefix = '_orig_mod.'\n",
    "for k, v in list(state_dict.items()):\n",
    "    if k.startswith(unwanted_prefix):\n",
    "        state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval()  # Set to evaluation mode\n",
    "model.to(device)\n",
    "\n",
    "print(f\"\\nModel loaded successfully!\")\n",
    "print(f\"Model is on device: {device}\")\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()) / 1e6:.0f}M\")\n",
    "\n",
    "# --- 4. Load the custom tokenizer ---\n",
    "class NanoTokenizer:\n",
    "    def __init__(self, cache_dir):\n",
    "        # We need to load the tokenizer from the nanochat package now\n",
    "        from nanochat.tokenizer import Tokenizer\n",
    "        tokenizer_path = cache_dir / 'tokenizer'\n",
    "        self.tokenizer_model = Tokenizer(str(tokenizer_path))\n",
    "    \n",
    "    def encode(self, text, bos=True, eos=True):\n",
    "        # Use the nanochat tokenizer's encode method signature\n",
    "        return self.tokenizer_model.encode(text, bos=bos, eos=eos)\n",
    "\n",
    "    def decode(self, tokens):\n",
    "        return self.tokenizer_model.decode(tokens)\n",
    "\n",
    "tokenizer = NanoTokenizer(cache_dir)\n",
    "print(\"Custom tokenizer loaded.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CSCE5720",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
