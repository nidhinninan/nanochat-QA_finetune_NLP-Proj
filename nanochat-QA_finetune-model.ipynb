{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "089de064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available. Using CUDA.\n",
      "Device Name: Quadro T2000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check if CUDA (NVIDIA GPU) is available and set the device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available. Using CUDA.\")\n",
    "    # You can also check the GPU name\n",
    "    print(f\"Device Name: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU not available, using CPU. This will be very slow.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12433605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell is now empty. The logic has been moved to the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71bfb0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "import pickle\n",
    "import json\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "# --- 1. Set up the nanochat environment ---\n",
    "if not Path('nanochat').exists():\n",
    "    print(\"Cloning karpathy/nanochat repository...\")\n",
    "    subprocess.run(['git', 'clone', 'https://github.com/karpathy/nanochat.git'], check=True)\n",
    "else:\n",
    "    print(\"nanochat repository already exists.\")\n",
    "\n",
    "# --- 1a. Install dependencies using uv ---\n",
    "print(\"\\nInstalling dependencies...\")\n",
    "!pip install -q uv\n",
    "%cd nanochat\n",
    "!uv sync\n",
    "%cd ..\n",
    "\n",
    "# Add the correct subfolder to the Python path to import its modules\n",
    "import sys\n",
    "# The actual package is inside the 'nanochat/nanochat' directory\n",
    "package_path = os.path.abspath('nanochat')\n",
    "if package_path not in sys.path:\n",
    "    sys.path.insert(0, package_path)\n",
    "\n",
    "# Import from the correct module within the package\n",
    "from nanochat.gpt import GPT, GPTConfig\n",
    "\n",
    "# --- 2. Manually download model and tokenizer files ---\n",
    "print(\"Downloading nanochat model and tokenizer files...\")\n",
    "model_repo = \"sdobson/nanochat\"\n",
    "cache_dir = Path.home() / '.cache' / 'nanochat'\n",
    "\n",
    "# Define files and their target directories within the cache\n",
    "files_to_download = {\n",
    "    \"model_000650.pt\": \"chatsft_checkpoints/d20/\",\n",
    "    \"meta_000650.json\": \"chatsft_checkpoints/d20/\",\n",
    "    \"tokenizer.pkl\": \"tokenizer/\",\n",
    "    \"token_bytes.pt\": \"tokenizer/\"\n",
    "}\n",
    "\n",
    "# Download all files to their respective cache directories\n",
    "for filename, target_subdir in files_to_download.items():\n",
    "    local_dir = cache_dir / target_subdir\n",
    "    os.makedirs(local_dir, exist_ok=True)\n",
    "    target_path = local_dir / filename\n",
    "    if not target_path.exists():\n",
    "        print(f\"Downloading {filename}...\")\n",
    "        hf_hub_download(repo_id=model_repo, filename=filename, local_dir=local_dir, local_dir_use_symlinks=False)\n",
    "    else:\n",
    "        print(f\"{filename} already exists. Skipping download.\")\n",
    "\n",
    "# --- 3. Load the model using nanochat's own scripts ---\n",
    "print(\"\\nLoading model using nanochat's native functions...\")\n",
    "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device = 'cpu'\n",
    "\n",
    "# Load the model checkpoint\n",
    "checkpoint_path = cache_dir / \"chatsft_checkpoints\" / \"d20\" / \"model_000650.pt\"\n",
    "meta_path = cache_dir / \"chatsft_checkpoints\" / \"d20\" / \"meta_000650.json\"\n",
    "\n",
    "state_dict = torch.load(checkpoint_path, map_location=device)\n",
    "\n",
    "with open(meta_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    meta = json.load(f)\n",
    "\n",
    "gptconf = GPTConfig(**meta['model_config'])\n",
    "model = GPT(gptconf)\n",
    "model.load_state_dict(state_dict, strict=True)\n",
    "\n",
    "# Fix the keys in the state_dict (remove '_orig_mod.' prefix)\n",
    "unwanted_prefix = '_orig_mod.'\n",
    "for k, v in list(state_dict.items()):\n",
    "    if k.startswith(unwanted_prefix):\n",
    "        state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval()  # Set to evaluation mode\n",
    "model.to(device)\n",
    "\n",
    "print(f\"\\nModel loaded successfully!\")\n",
    "print(f\"Model is on device: {device}\")\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()) / 1e6:.0f}M\")\n",
    "\n",
    "# --- 4. Load the custom tokenizer ---\n",
    "class NanoTokenizer:\n",
    "    def __init__(self, cache_dir):\n",
    "        # We need to load the tokenizer from the nanochat package now\n",
    "        from nanochat.tokenizer import Tokenizer\n",
    "        tokenizer_path = cache_dir / 'tokenizer'\n",
    "        self.tokenizer_model = Tokenizer(str(tokenizer_path))\n",
    "    \n",
    "    def encode(self, text, bos=True, eos=True):\n",
    "        # Use the nanochat tokenizer's encode method signature\n",
    "        return self.tokenizer_model.encode(text, bos=bos, eos=eos)\n",
    "\n",
    "    def decode(self, tokens):\n",
    "        return self.tokenizer_model.decode(tokens)\n",
    "\n",
    "tokenizer = NanoTokenizer(cache_dir)\n",
    "print(\"Custom tokenizer loaded.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CSCE5720",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
