{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "089de064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available. Using CUDA.\n",
      "Device Name: Quadro T2000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check if CUDA (NVIDIA GPU) is available and set the device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available. Using CUDA.\")\n",
    "    # You can also check the GPU name\n",
    "    print(f\"Device Name: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU not available, using CPU. This will be very slow.\")\n",
    "\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12433605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: uv in /home/nidhinninan/anaconda3/envs/CSCE5720/lib/python3.11/site-packages (0.9.8)\n",
      "/home/nidhinninan/Desktop/UCMO Classes/Natural Language Processing/Project/nanochat\n",
      "\u001b[2mResolved \u001b[1m91 packages\u001b[0m \u001b[2min 0.88ms\u001b[0m\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m71 packages\u001b[0m \u001b[2min 0.53ms\u001b[0m\u001b[0m\n",
      "/home/nidhinninan/Desktop/UCMO Classes/Natural Language Processing/Project\n",
      "Collecting tiktoken\n",
      "  Downloading tiktoken-0.12.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /home/nidhinninan/anaconda3/envs/CSCE5720/lib/python3.11/site-packages (from tiktoken) (2025.11.3)\n",
      "Requirement already satisfied: requests>=2.26.0 in /home/nidhinninan/anaconda3/envs/CSCE5720/lib/python3.11/site-packages (from tiktoken) (2.32.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/nidhinninan/anaconda3/envs/CSCE5720/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/nidhinninan/anaconda3/envs/CSCE5720/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/nidhinninan/anaconda3/envs/CSCE5720/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/nidhinninan/anaconda3/envs/CSCE5720/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (2025.10.5)\n",
      "Downloading tiktoken-0.12.0-cp311-cp311-manylinux_2_28_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tiktoken\n",
      "Successfully installed tiktoken-0.12.0\n"
     ]
    }
   ],
   "source": [
    "!pip install uv\n",
    "%cd nanochat\n",
    "!uv sync\n",
    "%cd ..\n",
    "!pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71bfb0df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nanochat repository already exists.\n",
      "Downloading nanochat model and tokenizer files...\n",
      "model_000650.pt already exists. Skipping download.\n",
      "meta_000650.json already exists. Skipping download.\n",
      "tokenizer.pkl already exists. Skipping download.\n",
      "token_bytes.pt already exists. Skipping download.\n",
      "\n",
      "Loading model using nanochat's native functions...\n",
      "\n",
      "Model loaded successfully!\n",
      "Model is on device: cpu\n",
      "Model parameters: 561M\n",
      "Custom tokenizer loaded.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "import pickle\n",
    "import json\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "# --- 1. Set up the nanochat environment ---\n",
    "if not Path('nanochat').exists():\n",
    "    print(\"Cloning karpathy/nanochat repository...\")\n",
    "    subprocess.run(['git', 'clone', 'https://github.com/karpathy/nanochat.git'], check=True)\n",
    "else:\n",
    "    print(\"nanochat repository already exists.\")\n",
    "\n",
    "# Add the correct subfolder to the Python path to import its modules\n",
    "import sys\n",
    "# The actual package is inside the 'nanochat/nanochat' directory\n",
    "package_path = os.path.abspath('nanochat')\n",
    "if package_path not in sys.path:\n",
    "    sys.path.insert(0, package_path)\n",
    "\n",
    "# Import from the correct module within the package\n",
    "from nanochat.gpt import GPT, GPTConfig\n",
    "\n",
    "# --- 2. Manually download model and tokenizer files ---\n",
    "print(\"Downloading nanochat model and tokenizer files...\")\n",
    "model_repo = \"sdobson/nanochat\"\n",
    "cache_dir = Path.home() / '.cache' / 'nanochat'\n",
    "\n",
    "# Define files and their target directories within the cache\n",
    "files_to_download = {\n",
    "    \"model_000650.pt\": \"chatsft_checkpoints/d20/\",\n",
    "    \"meta_000650.json\": \"chatsft_checkpoints/d20/\",\n",
    "    \"tokenizer.pkl\": \"tokenizer/\",\n",
    "    \"token_bytes.pt\": \"tokenizer/\"\n",
    "}\n",
    "\n",
    "# Download all files to their respective cache directories\n",
    "for filename, target_subdir in files_to_download.items():\n",
    "    local_dir = cache_dir / target_subdir\n",
    "    os.makedirs(local_dir, exist_ok=True)\n",
    "    target_path = local_dir / filename\n",
    "    if not target_path.exists():\n",
    "        print(f\"Downloading {filename}...\")\n",
    "        hf_hub_download(repo_id=model_repo, filename=filename, local_dir=local_dir, local_dir_use_symlinks=False)\n",
    "    else:\n",
    "        print(f\"{filename} already exists. Skipping download.\")\n",
    "\n",
    "# --- 3. Load the model using nanochat's own scripts ---\n",
    "print(\"\\nLoading model using nanochat's native functions...\")\n",
    "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device = 'cpu'\n",
    "\n",
    "# Load the model checkpoint\n",
    "checkpoint_path = cache_dir / \"chatsft_checkpoints\" / \"d20\" / \"model_000650.pt\"\n",
    "meta_path = cache_dir / \"chatsft_checkpoints\" / \"d20\" / \"meta_000650.json\"\n",
    "\n",
    "state_dict = torch.load(checkpoint_path, map_location=device)\n",
    "\n",
    "with open(meta_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    meta = json.load(f)\n",
    "\n",
    "gptconf = GPTConfig(**meta['model_config'])\n",
    "model = GPT(gptconf)\n",
    "model.load_state_dict(state_dict, strict=True)\n",
    "\n",
    "# Fix the keys in the state_dict (remove '_orig_mod.' prefix)\n",
    "unwanted_prefix = '_orig_mod.'\n",
    "for k, v in list(state_dict.items()):\n",
    "    if k.startswith(unwanted_prefix):\n",
    "        state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval()  # Set to evaluation mode\n",
    "model.to(device)\n",
    "\n",
    "print(f\"\\nModel loaded successfully!\")\n",
    "print(f\"Model is on device: {device}\")\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()) / 1e6:.0f}M\")\n",
    "\n",
    "# --- 4. Load the custom tokenizer ---\n",
    "class NanoTokenizer:\n",
    "    def __init__(self, cache_dir):\n",
    "        # We need to load the tokenizer from the nanochat package now\n",
    "        from nanochat.tokenizer import RustBPETokenizer\n",
    "        tokenizer_path = cache_dir / 'tokenizer'\n",
    "        self.tokenizer_model = RustBPETokenizer.from_directory(str(tokenizer_path))\n",
    "    \n",
    "    def encode(self, text, bos=True, eos=True):\n",
    "        # Use the nanochat tokenizer's encode method signature\n",
    "        return self.tokenizer_model.encode(text, bos=bos, eos=eos)\n",
    "\n",
    "    def decode(self, tokens):\n",
    "        return self.tokenizer_model.decode(tokens)\n",
    "\n",
    "tokenizer = NanoTokenizer(cache_dir)\n",
    "print(\"Custom tokenizer loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402abad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_repo = \"sdobson/nanochat\"\n",
    "base_cache = Path.home() / \".cache\" / \"nanochat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba12e36f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'RustBPETokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Initialize the nanochat tokenizer\u001b[39;00m\n\u001b[1;32m      2\u001b[0m tokenizer_dir \u001b[38;5;241m=\u001b[39m base_cache \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mRustBPETokenizer\u001b[49m\u001b[38;5;241m.\u001b[39mfrom_directory(\u001b[38;5;28mstr\u001b[39m(tokenizer_path))\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTokenizer ready.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'RustBPETokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "# Initialize the nanochat tokenizer\n",
    "tokenizer_dir = base_cache / \"tokenizer\"\n",
    "tokenizer = RustBPETokenizer.from_directory(str(tokenizer_path))\n",
    "\n",
    "print(\"Tokenizer ready.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CSCE5720",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
