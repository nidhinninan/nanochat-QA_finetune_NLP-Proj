{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 · Fine-Tune Nanochat Science QA Model\n",
    "\n",
    "This notebook configures and launches supervised fine-tuning for the Nanochat model on the ScienceQA conversational dataset prepared in the earlier steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "- Run `nanochat-QA_finetune.ipynb` to generate the `data/*_formatted` artifacts.\n",
    "- Run `02_load_base_model.ipynb` to cache the base `sdobson/nanochat` weights.\n",
    "- Ensure the required Python packages are installed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell-by-Cell Explanation of Changes\n",
    "\n",
    "**Removed Cell (Previously Cell 8): Environment Setup**\n",
    "```python\n",
    "!pip install uv\n",
    "%cd nanochat\n",
    "!uv sync\n",
    "%cd ..\n",
    "!pip install tiktoken\n",
    "```\n",
    "**Reason for Removal:** This cell is redundant. Environment setup and dependency installation were already performed in `02_load_base_model.ipynb`. Including it here is unnecessary and violates the principle of keeping notebooks focused on a single task. The fine-tuning notebook should assume the environment is correctly configured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "print(\"Checking GPU status with nvidia-smi (ignore errors if no NVIDIA GPU is available)...")\n",
    "try:\n",
    "    gpu_info = subprocess.check_output([\"nvidia-smi\"])  # noqa: S606 (intentional command execution)\n",
    "    print(gpu_info.decode())\n",
    "except (FileNotFoundError, subprocess.CalledProcessError) as exc:\n",
    "    print(f\"nvidia-smi unavailable: {exc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import torch\n",
    "\n",
    "project_dir = Path.cwd()\n",
    "data_dir = project_dir / \"data\"\n",
    "output_dir = project_dir / \"nanochat-science-finetuned\"\n",
    "final_dir = project_dir / \"nanochat-science-final\"\n",
    "\n",
    "print(f\"Project directory: {project_dir}\")\n",
    "print(f\"Data directory: {data_dir}\")\n",
    "print(f\"Output directory: {output_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"CUDA device detected: {torch.cuda.get_device_name(0)}\")\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Warning: Training on CPU will be extremely slow.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Prepared Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "train_dataset_path = data_dir / \"train_formatted\"\n",
    "val_dataset_path = data_dir / \"val_formatted\"\n",
    "\n",
    "if not train_dataset_path.exists():\n",
    "    raise FileNotFoundError(f\"Missing dataset: {train_dataset_path}. Run nanochat-QA_finetune.ipynb first.\")\n",
    "if not val_dataset_path.exists():\n",
    "    raise FileNotFoundError(f\"Missing dataset: {val_dataset_path}. Run nanochat-QA_finetune.ipynb first.\")\n",
    "\n",
    "train_dataset = load_from_disk(str(train_dataset_path))\n",
    "val_dataset = load_from_disk(str(val_dataset_path))\n",
    "\n",
    "print(train_dataset)\n",
    "print(val_dataset)\n",
    "print(f\"Train samples: {len(train_dataset)} | Validation samples: {len(val_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Base Model and Tokenizer\n",
    "\n",
    "This cell loads the model using the `transformers` library, which is required for compatibility with the `Trainer` API used for fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "\n",
    "model_name = \"sdobson/nanochat\"\n",
    "\n",
    "print(f\"Loading tokenizer and model: {model_name}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"right\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model.config.use_cache = False  # Required when using gradient checkpointing\n",
    "\n",
    "print(\"Model and tokenizer loaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize Conversations for Causal LM Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 512\n",
    "\n",
    "\n",
    "def conversation_to_text(messages):\n",
    "    \"\"\"Convert a chat-style dict of messages into a single training string.\"\"\"\n",
    "    segments = []\n",
    "    for message in messages:\n",
    "        role = message.get(\"role\", \"user\")\n",
    "        content = message.get(\"content\", \"\").strip()\n",
    "        segments.append(f\"{role}: {content}\")\n",
    "    return \"\\n\".join(segments)\n",
    "\n",
    "\n",
    "def tokenize_conversation(example):\n",
    "    text = conversation_to_text(example[\"messages\"])\n",
    "    return tokenizer(\n",
    "        text,\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        padding=False,\n",
    "        return_attention_mask=True,\n",
    "    )\n",
    "\n",
    "\n",
    "train_tokenized = train_dataset.map(\n",
    "    tokenize_conversation,\n",
    "    remove_columns=train_dataset.column_names,\n",
    "    desc=\"Tokenizing train dataset\",\n",
    ")\n",
    "\n",
    "val_tokenized = val_dataset.map(\n",
    "    tokenize_conversation,\n",
    "    remove_columns=val_dataset.column_names,\n",
    "    desc=\"Tokenizing validation dataset\",\n",
    ")\n",
    "\n",
    "print(train_tokenized)\n",
    "print(val_tokenized)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Data Collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,\n",
    ")\n",
    "\n",
    "print(\"Data collator initialized (causal LM mode).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "supports_bf16 = False\n",
    "if torch.cuda.is_available():\n",
    "    compute_capability = torch.cuda.get_device_capability()\n",
    "    supports_bf16 = compute_capability[0] >= 8\n",
    "\n",
    "run_name = f\"nanochat-science-ft-{datetime.utcnow().strftime('%Y%m%d-%H%M%S')}\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=str(output_dir),\n",
    "    run_name=run_name,\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=8,\n",
    "    learning_rate=5e-5,\n",
    "    warmup_steps=100,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=str(output_dir / \"logs\"),\n",
    "    logging_steps=50,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=200,\n",
    "    save_total_limit=2,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=200,\n",
    "    metric_for_best_model=\"loss\",\n",
    "    load_best_model_at_end=True,\n",
    "    fp16=torch.cuda.is_available() and not supports_bf16,\n",
    "    bf16=supports_bf16,\n",
    "    gradient_checkpointing=True,\n",
    "    report_to=\"none\",\n",
    "    dataloader_num_workers=2,\n",
    "    optim=\"adamw_torch\",\n",
    ")\n",
    "\n",
    "print(training_args)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if training_args.gradient_checkpointing:\n",
    "    model.gradient_checkpointing_enable()\n",
    "    model.enable_input_require_grads()  # Required for gradient checkpointing in newer HF versions\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_tokenized,\n",
    "    eval_dataset=val_tokenized,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "print(\"Trainer initialized.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_checkpoint = None  # Set to checkpoint path to resume training if interrupted.\n",
    "\n",
    "print(\"Starting fine-tuning — this may take several hours depending on GPU availability...\")\n",
    "train_result = trainer.train(resume_from_checkpoint=resume_checkpoint)\n",
    "trainer.save_state()\n",
    "\n",
    "trainer.log_metrics(\"train\", train_result.metrics)\n",
    "trainer.save_metrics(\"train\", train_result.metrics)\n",
    "\n",
    "print(\"Training complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Best Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_metrics = trainer.evaluate()\n",
    "trainer.log_metrics(\"eval\", eval_metrics)\n",
    "trainer.save_metrics(\"eval\", eval_metrics)\n",
    "\n",
    "print(eval_metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Final Model Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "trainer.save_model(str(final_dir))\n",
    "tokenizer.save_pretrained(str(final_dir))\n",
    "\n",
    "metrics_summary = {\n",
    "    \"train\": train_result.metrics,\n",
    "    \"eval\": eval_metrics if \"eval_metrics\" in locals() else None,\n",
    "}\n",
    "\n",
    "metrics_path = final_dir / \"training_metrics.json\"\n",
    "with metrics_path.open(\"w\") as fp:\n",
    "    json.dump(metrics_summary, fp, indent=2)\n",
    "\n",
    "print(f\"Saved fine-tuned model and tokenizer to {final_dir}\")\n",
    "print(f\"Metrics written to {metrics_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Push to Hugging Face Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from huggingface_hub import HfApi\n",
    "#\n",
    "# repo_id = \"your-username/nanochat-science-qa\"\n",
    "# api = HfApi()\n",
    "# api.create_repo(repo_id=repo_id, exist_ok=True)\n",
    "# trainer.push_to_hub()\n",
    "# tokenizer.push_to_hub(repo_id)\n",
    "\n",
    "print(\"Configure and uncomment the cell above to push the model to the Hugging Face Hub.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Steps\n",
    "\n",
    "- Validate the fine-tuned model in `04_evaluation.ipynb`.\n",
    "- Build interactive demos in `05_interactive_demo.ipynb`.\n",
    "- Document cost, runtime, and findings for the final report.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diff for `02_load_base_model.ipynb`\n",
    "\n",
    "As requested, here are the potential diffs if you wanted to persist variables from `02_load_base_model.ipynb` to this notebook. However, for the `transformers.Trainer` to work, the model must be loaded with `AutoModelForCausalLM`, which this notebook already does. The custom loading method in notebook 02 is incompatible. Therefore, no changes are strictly necessary. These diffs are for your information only.\n",
    "\n",
    "To save the model and tokenizer in a transformers-compatible format from notebook 02, you would add this cell at the end:\n",
    "\n",
    "```diff\n",
    "+    # Save model and tokenizer in a transformers-compatible format\n",
    "+    model_save_path = \"./nanochat_base_model_transformers\"\n",
    "+    model.save_pretrained(model_save_path)\n",
    "+    tokenizer.save_pretrained(model_save_path)\n",
    "+    print(f\"Model and tokenizer saved to {model_save_path})
",
    "```\n",
    "\n",
    "Then, in this notebook (`03_finetune_model.ipynb`), you would load it like this, replacing the Hugging Face download:\n",
    "\n",
    "```diff\n",
    "-\n",
    "model_name = \"sdobson/nanochat\"\n",
    "+\n",
    "model_name = \"./nanochat_base_model_transformers\"\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}