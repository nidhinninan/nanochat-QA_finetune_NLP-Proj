{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Notebook 2 Â· Load Base `nanochat` Checkpoint\n",
        "\n",
        "This notebook loads the `sdobson/nanochat` checkpoint, evaluates its baseline performance on the prepared ScienceQA subset, and saves the raw baseline responses for later comparison.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu (CPU)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "import json\n",
        "import subprocess\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "from datasets import load_from_disk\n",
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "# Detect available device (CUDA preferred)\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    device_name = torch.cuda.get_device_name(0)\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    device_name = \"CPU\"\n",
        "\n",
        "device = torch.device(\"cpu\")\n",
        "device_name = \"CPU\"\n",
        "\n",
        "print(f\"Using device: {device} ({device_name})\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: uv in /home/nidhinninan/anaconda3/envs/CSCE5720/lib/python3.11/site-packages (0.9.8)\n",
            "/home/nidhinninan/Desktop/UCMO Classes/Natural Language Processing/Project/nanochat\n",
            "\u001b[2mResolved \u001b[1m91 packages\u001b[0m \u001b[2min 0.77ms\u001b[0m\u001b[0m\n",
            "\u001b[2mAudited \u001b[1m71 packages\u001b[0m \u001b[2min 8ms\u001b[0m\u001b[0m\n",
            "/home/nidhinninan/Desktop/UCMO Classes/Natural Language Processing/Project\n",
            "Requirement already satisfied: tiktoken in /home/nidhinninan/anaconda3/envs/CSCE5720/lib/python3.11/site-packages (0.12.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /home/nidhinninan/anaconda3/envs/CSCE5720/lib/python3.11/site-packages (from tiktoken) (2025.11.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /home/nidhinninan/anaconda3/envs/CSCE5720/lib/python3.11/site-packages (from tiktoken) (2.32.5)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /home/nidhinninan/anaconda3/envs/CSCE5720/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/nidhinninan/anaconda3/envs/CSCE5720/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/nidhinninan/anaconda3/envs/CSCE5720/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (1.26.20)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/nidhinninan/anaconda3/envs/CSCE5720/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (2025.10.5)\n"
          ]
        }
      ],
      "source": [
        "!pip install uv\n",
        "%cd nanochat\n",
        "!uv sync\n",
        "%cd ..\n",
        "!pip install tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "nanochat repository already present.\n"
          ]
        }
      ],
      "source": [
        "# Clone the nanochat repository (idempotent)\n",
        "nanochat_repo = Path(\"nanochat\")\n",
        "if not nanochat_repo.exists():\n",
        "    print(\"Cloning karpathy/nanochat...\")\n",
        "    subprocess.run([\"git\", \"clone\", \"https://github.com/karpathy/nanochat.git\", str(nanochat_repo)], check=True)\n",
        "else:\n",
        "    print(\"nanochat repository already present.\")\n",
        "\n",
        "# Ensure the nanochat package is importable\n",
        "package_path = nanochat_repo.resolve()\n",
        "if str(package_path) not in sys.path:\n",
        "    sys.path.insert(0, str(package_path))\n",
        "\n",
        "from nanochat.gpt import GPT, GPTConfig\n",
        "from nanochat.tokenizer import RustBPETokenizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found cached model_000650.pt.\n",
            "Found cached meta_000650.json.\n",
            "Found cached tokenizer.pkl.\n",
            "Found cached token_bytes.pt.\n"
          ]
        }
      ],
      "source": [
        "# Download checkpoint artifacts from Hugging Face (if needed)\n",
        "model_repo = \"sdobson/nanochat\"\n",
        "base_cache = Path.home() / \".cache\" / \"nanochat\"\n",
        "\n",
        "files_to_download = {\n",
        "    \"model_000650.pt\": base_cache / \"chatsft_checkpoints\" / \"d20\",\n",
        "    \"meta_000650.json\": base_cache / \"chatsft_checkpoints\" / \"d20\",\n",
        "    \"tokenizer.pkl\": base_cache / \"tokenizer\",\n",
        "    \"token_bytes.pt\": base_cache / \"tokenizer\",\n",
        "}\n",
        "\n",
        "for filename, target_dir in files_to_download.items():\n",
        "    target_dir.mkdir(parents=True, exist_ok=True)\n",
        "    target_path = target_dir / filename\n",
        "    if not target_path.exists():\n",
        "        print(f\"Downloading {filename}...\")\n",
        "        hf_hub_download(\n",
        "            repo_id=model_repo,\n",
        "            filename=filename,\n",
        "            local_dir=str(target_dir),\n",
        "            local_dir_use_symlinks=False,\n",
        "        )\n",
        "    else:\n",
        "        print(f\"Found cached {filename}.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded nanochat base model.\n",
            "Model is on device: cpu\n",
            "Model parameters: 561M\n"
          ]
        }
      ],
      "source": [
        "# Load checkpoint weights and configuration\n",
        "checkpoint_dir = base_cache / \"chatsft_checkpoints\" / \"d20\"\n",
        "model_path = checkpoint_dir / \"model_000650.pt\"\n",
        "meta_path = checkpoint_dir / \"meta_000650.json\"\n",
        "\n",
        "torch_dtype = torch.float16 if device.type == \"cuda\" else torch.float32\n",
        "state_dict = torch.load(model_path, map_location=device)\n",
        "\n",
        "with open(meta_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    meta = json.load(f)\n",
        "\n",
        "model_config = GPTConfig(**meta[\"model_config\"])\n",
        "model = GPT(model_config)\n",
        "\n",
        "# Clean state dict keys (strip _orig_mod.)\n",
        "if any(k.startswith(\"_orig_mod.\") for k in state_dict.keys()):\n",
        "    state_dict = {k.removeprefix(\"_orig_mod.\"): v for k, v in state_dict.items()}\n",
        "\n",
        "# Move model to device and load weights\n",
        "model.to(device)\n",
        "model.load_state_dict(state_dict, strict=True)\n",
        "model.eval()\n",
        "\n",
        "print(\"Loaded nanochat base model.\")\n",
        "print(f\"Model is on device: {device}\")\n",
        "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()) / 1e6:.0f}M\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokenizer ready.\n"
          ]
        }
      ],
      "source": [
        "# Initialize the nanochat tokenizer\n",
        "tokenizer_dir = base_cache / \"tokenizer\"\n",
        "tokenizer = RustBPETokenizer.from_directory(str(tokenizer_dir))\n",
        "\n",
        "print(\"Tokenizer ready.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Precompute frequently used special token IDs\n",
        "bos_id = tokenizer.get_bos_token_id()\n",
        "assistant_start_id = tokenizer.encode_special(\"<|assistant_start|>\")\n",
        "assistant_end_id = tokenizer.encode_special(\"<|assistant_end|>\")\n",
        "user_start_id = tokenizer.encode_special(\"<|user_start|>\")\n",
        "user_end_id = tokenizer.encode_special(\"<|user_end|>\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_completion_prompt(conversation):\n",
        "    \"\"\"Return token ids primed for assistant completion.\"\"\"\n",
        "    conversation_for_completion = json.loads(json.dumps(conversation))  # deep copy via json\n",
        "    conversation_for_completion[\"messages\"][-1][\"content\"] = \"\"\n",
        "    prompt_tokens = tokenizer.render_for_completion(conversation_for_completion)\n",
        "    return prompt_tokens\n",
        "\n",
        "\n",
        "def generate_response_from_tokens(prompt_tokens, *, max_tokens=256, temperature=0.7, top_k=50):\n",
        "    tokens = list(prompt_tokens)\n",
        "    generated = []\n",
        "    for token in model.generate(tokens, max_tokens=max_tokens, temperature=temperature, top_k=top_k):\n",
        "        tokens.append(token)\n",
        "        generated.append(token)\n",
        "        if token == assistant_end_id:\n",
        "            break\n",
        "    text = tokenizer.decode([token for token in generated if token not in {assistant_end_id}])\n",
        "    return text.strip(), generated\n",
        "\n",
        "\n",
        "def generate_response_for_conversation(conversation, **generation_kwargs):\n",
        "    prompt_tokens = build_completion_prompt(conversation)\n",
        "    text, token_sequence = generate_response_from_tokens(prompt_tokens, **generation_kwargs)\n",
        "    return {\n",
        "        \"prompt_tokens\": prompt_tokens,\n",
        "        \"generated_tokens\": token_sequence,\n",
        "        \"response_text\": text,\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded formatted test subset with 500 examples.\n"
          ]
        }
      ],
      "source": [
        "# Load PRE-FORMATTED ScienceQA evaluation subset\n",
        "DATA_DIR = Path(\"data\")\n",
        "\n",
        "try:\n",
        "    # Load the data that was already processed in Notebook 1\n",
        "    test_formatted = load_from_disk(str(DATA_DIR / \"test_subset\"))\n",
        "    print(f\"Loaded formatted test subset with {len(test_formatted)} examples.\")\n",
        "except FileNotFoundError:\n",
        "    test_formatted = None\n",
        "    print(\"Warning: data/test_formatted not found. Please run Notebook 1 first and save the formatted data.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded formatted test subset with 500 examples.\n",
            "Prepared 500 conversations from pre-processed file.\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_from_disk\n",
        "from pathlib import Path\n",
        "\n",
        "test_formatted = load_from_disk(str(DATA_DIR / \"test_formatted\"))\n",
        "print(f\"Loaded formatted test subset with {len(test_formatted)} examples.\")\n",
        "\n",
        "conversations = []\n",
        "for example in test_formatted:\n",
        "    messages = example[\"messages\"]  # now present\n",
        "    conversations.append({\n",
        "        \"id\": example.get(\"id\"),\n",
        "        \"conversation\": {\"messages\": messages},\n",
        "        \"expected_response\": messages[2][\"content\"],\n",
        "    })\n",
        "print(f\"Prepared {len(conversations)} conversations from pre-processed file.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prepared 500 conversations from pre-processed file.\n"
          ]
        }
      ],
      "source": [
        "# Prepare conversations from the pre-formatted file\n",
        "if test_formatted is not None:\n",
        "    conversations = []\n",
        "    for example in test_formatted:\n",
        "        # The assistant's content is the expected response\n",
        "        conversations.append({\n",
        "            \"id\": example.get(\"id\"), # We can still get the ID\n",
        "            \"conversation\": {\"messages\": example[\"messages\"]},\n",
        "            \"expected_response\": example[\"messages\"][2][\"content\"], # 2 is the assistant's message\n",
        "        })\n",
        "    print(f\"Prepared {len(conversations)} conversations from pre-processed file.\")\n",
        "else:\n",
        "    conversations = []\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate baseline responses (configure sample size as needed)\n",
        "EVAL_SAMPLE_SIZE = 50  # adjust to cover more/less examples\n",
        "\n",
        "def evaluate_baseline(sample_size=EVAL_SAMPLE_SIZE, temperature=0.7, top_k=50):\n",
        "    assert conversations, \"No conversations prepared. Ensure Notebook 1 has been run.\"\n",
        "    results = []\n",
        "    for idx, example in enumerate(conversations[:sample_size]):\n",
        "        convo = example[\"conversation\"]\n",
        "        generation = generate_response_for_conversation(\n",
        "            convo,\n",
        "            max_tokens=256,\n",
        "            temperature=temperature,\n",
        "            top_k=top_k,\n",
        "        )\n",
        "        results.append({\n",
        "            \"index\": idx,\n",
        "            \"example_id\": example.get(\"id\"),\n",
        "            \"question\": convo[\"messages\"][1][\"content\"],\n",
        "            \"expected\": example[\"expected_response\"],\n",
        "            \"response\": generation[\"response_text\"],\n",
        "            \"prompt_tokens\": generation[\"prompt_tokens\"],\n",
        "            \"generated_tokens\": generation[\"generated_tokens\"],\n",
        "        })\n",
        "    return results\n",
        "\n",
        "# baseline_results = evaluate_baseline()\n",
        "# len(baseline_results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_choice_letter(text):\n",
        "    for letter in [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\"]:\n",
        "        if f\"{letter}.\" in text:\n",
        "            return letter\n",
        "        if f\" {letter} \" in text:\n",
        "            return letter\n",
        "    return None\n",
        "\n",
        "\n",
        "def compute_accuracy(results):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for entry in results:\n",
        "        expected_letter = extract_choice_letter(entry[\"expected\"])\n",
        "        predicted_letter = extract_choice_letter(entry[\"response\"])\n",
        "        if expected_letter and predicted_letter:\n",
        "            total += 1\n",
        "            if expected_letter == predicted_letter:\n",
        "                correct += 1\n",
        "    accuracy = correct / total if total else 0.0\n",
        "    return {\n",
        "        \"evaluated\": len(results),\n",
        "        \"scored\": total,\n",
        "        \"correct\": correct,\n",
        "        \"accuracy\": accuracy,\n",
        "    }\n",
        "\n",
        "# metrics = compute_accuracy(baseline_results)\n",
        "# metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "BASELINE_OUTPUT_PATH = Path(\"baseline_responses.json\")\n",
        "\n",
        "\n",
        "def save_baseline_results(results, path=BASELINE_OUTPUT_PATH):\n",
        "    path = Path(path)\n",
        "    with path.open(\"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(results, f, indent=2, ensure_ascii=False)\n",
        "    print(f\"Saved {len(results)} baseline responses to {path}.\")\n",
        "\n",
        "# save_baseline_results(baseline_results)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "def summarize_result(entry, idx=0):\n",
        "    print(f\"Example {idx}\")\n",
        "    print(\"Question:\\n\", entry[\"question\"])\n",
        "    print(\"\\nExpected:\\n\", entry[\"expected\"])\n",
        "    print(\"\\nModel Response:\\n\", entry[\"response\"])\n",
        "\n",
        "# if baseline_results:\n",
        "#     summarize_result(baseline_results[0], idx=baseline_results[0][\"index\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Next Steps\n",
        "\n",
        "1. Uncomment the evaluation cells to generate baseline responses once the ScienceQA subsets are prepared.\n",
        "2. Review `baseline_responses.json` to confirm output quality before starting fine-tuning.\n",
        "3. Keep an eye on GPU availability; running on CPU will be slow for the full evaluation set.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "CSCE5720",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
