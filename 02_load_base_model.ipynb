{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Notebook 2 · Load Base `nanochat` Checkpoint\n",
        "\n",
        "This notebook loads the `sdobson/nanochat` checkpoint, evaluates its baseline performance on the prepared ScienceQA subset, and saves the raw baseline responses for later comparison.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "nanochat repository already present.\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Clone the nanochat repository (idempotent)\n",
        "nanochat_repo = Path(\"nanochat\")\n",
        "if not nanochat_repo.exists():\n",
        "    print(\"Cloning karpathy/nanochat...\")\n",
        "    subprocess.run([\"git\", \"clone\", \"https://github.com/karpathy/nanochat.git\", str(nanochat_repo)], check=True)\n",
        "else:\n",
        "    print(\"nanochat repository already present.\")\n",
        "\n",
        "# Ensure the nanochat package is importable\n",
        "package_path = nanochat_repo.resolve()\n",
        "if str(package_path) not in sys.path:\n",
        "    sys.path.insert(0, str(package_path))\n",
        "\n",
        "from nanochat.gpt import GPT, GPTConfig\n",
        "from nanochat.tokenizer import RustBPETokenizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Loading model using nanochat's native functions...\n"
          ]
        }
      ],
      "source": [
        "# Load the model using nanochat's own scripts ---\n",
        "print(\"\\nLoading model using nanochat's native functions...\")\n",
        "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device = 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found cached model_000650.pt.\n",
            "Found cached meta_000650.json.\n",
            "Found cached tokenizer.pkl.\n",
            "Found cached token_bytes.pt.\n"
          ]
        }
      ],
      "source": [
        "# Download checkpoint artifacts from Hugging Face (if needed)\n",
        "model_repo = \"sdobson/nanochat\"\n",
        "base_cache = Path.home() / \".cache\" / \"nanochat\"\n",
        "\n",
        "files_to_download = {\n",
        "    \"model_000650.pt\": base_cache / \"chatsft_checkpoints\" / \"d20\",\n",
        "    \"meta_000650.json\": base_cache / \"chatsft_checkpoints\" / \"d20\",\n",
        "    \"tokenizer.pkl\": base_cache / \"tokenizer\",\n",
        "    \"token_bytes.pt\": base_cache / \"tokenizer\",\n",
        "}\n",
        "\n",
        "for filename, target_dir in files_to_download.items():\n",
        "    target_dir.mkdir(parents=True, exist_ok=True)\n",
        "    target_path = target_dir / filename\n",
        "    if not target_path.exists():\n",
        "        print(f\"Downloading {filename}...\")\n",
        "        hf_hub_download(\n",
        "            repo_id=model_repo,\n",
        "            filename=filename,\n",
        "            local_dir=str(target_dir),\n",
        "            local_dir_use_symlinks=False,\n",
        "        )\n",
        "    else:\n",
        "        print(f\"Found cached {filename}.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "nanochat repository already exists.\n",
            "Downloading nanochat model and tokenizer files...\n",
            "model_000650.pt already exists. Skipping download.\n",
            "meta_000650.json already exists. Skipping download.\n",
            "tokenizer.pkl already exists. Skipping download.\n",
            "token_bytes.pt already exists. Skipping download.\n",
            "\n",
            "Loading model using nanochat's native functions...\n",
            "\n",
            "Model loaded successfully!\n",
            "Model is on device: cpu\n",
            "Model parameters: 561M\n",
            "Custom tokenizer loaded.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import os\n",
        "from pathlib import Path\n",
        "import subprocess\n",
        "import pickle\n",
        "import json\n",
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "# --- 1. Set up the nanochat environment ---\n",
        "if not Path('nanochat').exists():\n",
        "    print(\"Cloning karpathy/nanochat repository...\")\n",
        "    subprocess.run(['git', 'clone', 'https://github.com/karpathy/nanochat.git'], check=True)\n",
        "else:\n",
        "    print(\"nanochat repository already exists.\")\n",
        "\n",
        "# Add the correct subfolder to the Python path to import its modules\n",
        "import sys\n",
        "# The actual package is inside the 'nanochat/nanochat' directory\n",
        "package_path = os.path.abspath('nanochat')\n",
        "if package_path not in sys.path:\n",
        "    sys.path.insert(0, package_path)\n",
        "\n",
        "# Import from the correct module within the package\n",
        "from nanochat.gpt import GPT, GPTConfig\n",
        "\n",
        "# --- 2. Manually download model and tokenizer files ---\n",
        "print(\"Downloading nanochat model and tokenizer files...\")\n",
        "model_repo = \"sdobson/nanochat\"\n",
        "cache_dir = Path.home() / '.cache' / 'nanochat'\n",
        "\n",
        "# Define files and their target directories within the cache\n",
        "files_to_download = {\n",
        "    \"model_000650.pt\": \"chatsft_checkpoints/d20/\",\n",
        "    \"meta_000650.json\": \"chatsft_checkpoints/d20/\",\n",
        "    \"tokenizer.pkl\": \"tokenizer/\",\n",
        "    \"token_bytes.pt\": \"tokenizer/\"\n",
        "}\n",
        "\n",
        "# Download all files to their respective cache directories\n",
        "for filename, target_subdir in files_to_download.items():\n",
        "    local_dir = cache_dir / target_subdir\n",
        "    os.makedirs(local_dir, exist_ok=True)\n",
        "    target_path = local_dir / filename\n",
        "    if not target_path.exists():\n",
        "        print(f\"Downloading {filename}...\")\n",
        "        hf_hub_download(repo_id=model_repo, filename=filename, local_dir=local_dir, local_dir_use_symlinks=False)\n",
        "    else:\n",
        "        print(f\"{filename} already exists. Skipping download.\")\n",
        "\n",
        "# --- 3. Load the model using nanochat's own scripts ---\n",
        "print(\"\\nLoading model using nanochat's native functions...\")\n",
        "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device = 'cpu'\n",
        "\n",
        "# Load the model checkpoint\n",
        "checkpoint_path = cache_dir / \"chatsft_checkpoints\" / \"d20\" / \"model_000650.pt\"\n",
        "meta_path = cache_dir / \"chatsft_checkpoints\" / \"d20\" / \"meta_000650.json\"\n",
        "\n",
        "state_dict = torch.load(checkpoint_path, map_location=device)\n",
        "\n",
        "with open(meta_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    meta = json.load(f)\n",
        "\n",
        "gptconf = GPTConfig(**meta['model_config'])\n",
        "model = GPT(gptconf)\n",
        "model.load_state_dict(state_dict, strict=True)\n",
        "\n",
        "# Fix the keys in the state_dict (remove '_orig_mod.' prefix)\n",
        "unwanted_prefix = '_orig_mod.'\n",
        "for k, v in list(state_dict.items()):\n",
        "    if k.startswith(unwanted_prefix):\n",
        "        state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
        "\n",
        "model.load_state_dict(state_dict)\n",
        "model.eval()  # Set to evaluation mode\n",
        "model.to(device)\n",
        "\n",
        "print(f\"\\nModel loaded successfully!\")\n",
        "print(f\"Model is on device: {device}\")\n",
        "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()) / 1e6:.0f}M\")\n",
        "\n",
        "# --- 4. Load the custom tokenizer ---\n",
        "class NanoTokenizer:\n",
        "    def __init__(self, cache_dir):\n",
        "        # We need to load the tokenizer from the nanochat package now\n",
        "        from nanochat.tokenizer import RustBPETokenizer\n",
        "        tokenizer_path = cache_dir / 'tokenizer'\n",
        "        self.tokenizer_model = RustBPETokenizer.from_directory(str(tokenizer_path))\n",
        "    \n",
        "    def encode(self, text, bos=True, eos=True):\n",
        "        # Use the nanochat tokenizer's encode method signature\n",
        "        return self.tokenizer_model.encode(text, bos=bos, eos=eos)\n",
        "\n",
        "    def decode(self, tokens):\n",
        "        return self.tokenizer_model.decode(tokens)\n",
        "\n",
        "# tokenizer = NanoTokenizer(cache_dir)\n",
        "# print(\"Custom tokenizer loaded.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokenizer ready.\n"
          ]
        }
      ],
      "source": [
        "# Initialize the nanochat tokenizer\n",
        "tokenizer_dir = base_cache / \"tokenizer\"\n",
        "tokenizer = RustBPETokenizer.from_directory(str(tokenizer_dir))\n",
        "\n",
        "print(\"Tokenizer ready.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Precompute frequently used special token IDs\n",
        "bos_id = tokenizer.get_bos_token_id()\n",
        "assistant_start_id = tokenizer.encode_special(\"<|assistant_start|>\")\n",
        "assistant_end_id = tokenizer.encode_special(\"<|assistant_end|>\")\n",
        "user_start_id = tokenizer.encode_special(\"<|user_start|>\")\n",
        "user_end_id = tokenizer.encode_special(\"<|user_end|>\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_completion_prompt(conversation):\n",
        "    \"\"\"Return token ids primed for assistant completion.\"\"\"\n",
        "    conversation_for_completion = json.loads(json.dumps(conversation))  # deep copy via json\n",
        "    conversation_for_completion[\"messages\"][-1][\"content\"] = \"\"\n",
        "    prompt_tokens = tokenizer.render_for_completion(conversation_for_completion)\n",
        "    return prompt_tokens\n",
        "\n",
        "\n",
        "def generate_response_from_tokens(prompt_tokens, *, max_tokens=256, temperature=0.7, top_k=50):\n",
        "    tokens = list(prompt_tokens)\n",
        "    generated = []\n",
        "    for token in model.generate(tokens, max_tokens=max_tokens, temperature=temperature, top_k=top_k):\n",
        "        tokens.append(token)\n",
        "        generated.append(token)\n",
        "        if token == assistant_end_id:\n",
        "            break\n",
        "    text = tokenizer.decode([token for token in generated if token not in {assistant_end_id}])\n",
        "    return text.strip(), generated\n",
        "\n",
        "\n",
        "def generate_response_for_conversation(conversation, **generation_kwargs):\n",
        "    prompt_tokens = build_completion_prompt(conversation)\n",
        "    text, token_sequence = generate_response_from_tokens(prompt_tokens, **generation_kwargs)\n",
        "    return {\n",
        "        \"prompt_tokens\": prompt_tokens,\n",
        "        \"generated_tokens\": token_sequence,\n",
        "        \"response_text\": text,\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded formatted test subset with 500 examples.\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_from_disk\n",
        "from pathlib import Path\n",
        "\n",
        "# Load PRE-FORMATTED ScienceQA evaluation subset\n",
        "DATA_DIR = Path(\"data\")\n",
        "\n",
        "try:\n",
        "    # Load the data that was already processed in Notebook 1\n",
        "    test_formatted = load_from_disk(str(DATA_DIR / \"test_subset\"))\n",
        "    print(f\"Loaded formatted test subset with {len(test_formatted)} examples.\")\n",
        "except FileNotFoundError:\n",
        "    test_formatted = None\n",
        "    print(\"Warning: data/test_formatted not found. Please run Notebook 1 first and save the formatted data.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded formatted test subset with 500 examples.\n",
            "Prepared 500 conversations from pre-processed file.\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_from_disk\n",
        "from pathlib import Path\n",
        "\n",
        "test_formatted = load_from_disk(str(DATA_DIR / \"test_formatted\"))\n",
        "print(f\"Loaded formatted test subset with {len(test_formatted)} examples.\")\n",
        "\n",
        "conversations = []\n",
        "for example in test_formatted:\n",
        "    messages = example[\"messages\"]  # now present\n",
        "    conversations.append({\n",
        "        \"id\": example.get(\"id\"),\n",
        "        \"conversation\": {\"messages\": messages},\n",
        "        \"expected_response\": messages[2][\"content\"],\n",
        "    })\n",
        "print(f\"Prepared {len(conversations)} conversations from pre-processed file.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prepared 500 conversations from pre-processed file.\n"
          ]
        }
      ],
      "source": [
        "# Prepare conversations from the pre-formatted file\n",
        "if test_formatted is not None:\n",
        "    conversations = []\n",
        "    for example in test_formatted:\n",
        "        # The assistant's content is the expected response\n",
        "        conversations.append({\n",
        "            \"id\": example.get(\"id\"), # We can still get the ID\n",
        "            \"conversation\": {\"messages\": example[\"messages\"]},\n",
        "            \"expected_response\": example[\"messages\"][2][\"content\"], # 2 is the assistant's message\n",
        "        })\n",
        "    print(f\"Prepared {len(conversations)} conversations from pre-processed file.\")\n",
        "else:\n",
        "    conversations = []\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate baseline responses (configure sample size as needed)\n",
        "EVAL_SAMPLE_SIZE = 50  # adjust to cover more/less examples\n",
        "\n",
        "def evaluate_baseline(sample_size=EVAL_SAMPLE_SIZE, temperature=0.7, top_k=50):\n",
        "    assert conversations, \"No conversations prepared. Ensure Notebook 1 has been run.\"\n",
        "    results = []\n",
        "    for idx, example in enumerate(conversations[:sample_size]):\n",
        "        convo = example[\"conversation\"]\n",
        "        generation = generate_response_for_conversation(\n",
        "            convo,\n",
        "            max_tokens=256,\n",
        "            temperature=temperature,\n",
        "            top_k=top_k,\n",
        "        )\n",
        "        results.append({\n",
        "            \"index\": idx,\n",
        "            \"example_id\": example.get(\"id\"),\n",
        "            \"question\": convo[\"messages\"][1][\"content\"],\n",
        "            \"expected\": example[\"expected_response\"],\n",
        "            \"response\": generation[\"response_text\"],\n",
        "            \"prompt_tokens\": generation[\"prompt_tokens\"],\n",
        "            \"generated_tokens\": generation[\"generated_tokens\"],\n",
        "        })\n",
        "    return results\n",
        "\n",
        "# baseline_results = evaluate_baseline()\n",
        "# len(baseline_results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_choice_letter(text):\n",
        "    for letter in [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\"]:\n",
        "        if f\"{letter}.\" in text:\n",
        "            return letter\n",
        "        if f\" {letter} \" in text:\n",
        "            return letter\n",
        "    return None\n",
        "\n",
        "\n",
        "def compute_accuracy(results):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for entry in results:\n",
        "        expected_letter = extract_choice_letter(entry[\"expected\"])\n",
        "        predicted_letter = extract_choice_letter(entry[\"response\"])\n",
        "        if expected_letter and predicted_letter:\n",
        "            total += 1\n",
        "            if expected_letter == predicted_letter:\n",
        "                correct += 1\n",
        "    accuracy = correct / total if total else 0.0\n",
        "    return {\n",
        "        \"evaluated\": len(results),\n",
        "        \"scored\": total,\n",
        "        \"correct\": correct,\n",
        "        \"accuracy\": accuracy,\n",
        "    }\n",
        "\n",
        "# metrics = compute_accuracy(baseline_results)\n",
        "# metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [],
      "source": [
        "def summarize_result(entry, idx=0):\n",
        "    print(f\"Example {idx}\")\n",
        "    print(\"Question:\\n\", entry[\"question\"])\n",
        "    print(\"\\nExpected:\\n\", entry[\"expected\"])\n",
        "    print(\"\\nModel Response:\\n\", entry[\"response\"])\n",
        "\n",
        "# if baseline_results:\n",
        "#     summarize_result(baseline_results[0], idx=baseline_results[0][\"index\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Next Steps\n",
        "\n",
        "1. Uncomment the evaluation cells to generate baseline responses once the ScienceQA subsets are prepared.\n",
        "2. Review `baseline_responses.json` to confirm output quality before starting fine-tuning.\n",
        "3. Keep an eye on GPU availability; running on CPU will be slow for the full evaluation set.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data generator `sft_data_generator` is defined.\n"
          ]
        }
      ],
      "source": [
        "# This is the data loader from scripts/chat_sft.py\n",
        "def sft_data_generator(dataset, batch_size):\n",
        "    pad_token_id = tokenizer.encode_special(\"<|assistant_end|>\") # use <|assistant_end|> as the pad token is ok, these positions are masked in the loss\n",
        "    \n",
        "    # prepares a list of tokenized conversations into a batch and yields\n",
        "    def collate_and_yield(batch):\n",
        "        nrows = len(batch)\n",
        "        ncols = max(len(ids) for ids, mask in batch) - 1 # seq of n creates inputs/targets of n-1\n",
        "        inputs = torch.full((nrows, ncols), pad_token_id, dtype=torch.long)\n",
        "        targets = torch.full((nrows, ncols), -1, dtype=torch.long) # -1 is ignore index\n",
        "        for i, (ids, mask) in enumerate(batch):\n",
        "            n = len(ids)\n",
        "            ids_tensor = torch.tensor(ids, dtype=torch.long)\n",
        "            inputs[i, :n-1] = ids_tensor[:-1]\n",
        "            # recall -1 is the ignore index, so mask out targets where mask is 0\n",
        "            row_targets = ids_tensor[1:]\n",
        "            # mask[1:] omits the mask for the BOS token, which is never a target atm so it's ok\n",
        "            mask_tensor = torch.tensor(mask[1:], dtype=torch.long)\n",
        "            row_targets[mask_tensor == 0] = -1 # mask out targets where mask is 0\n",
        "            targets[i, :n-1] = row_targets\n",
        "        inputs = inputs.to(device) # move to device\n",
        "        targets = targets.to(device)\n",
        "        return inputs, targets\n",
        "    \n",
        "    # iterates over the dataset in epochs, tokenizes\n",
        "    batch = []\n",
        "    while True:\n",
        "        for i in range(len(dataset)):\n",
        "            doc = dataset[i]\n",
        "            ids, mask = tokenizer.render_conversation(doc)\n",
        "            batch.append((ids, mask))\n",
        "            if len(batch) == batch_size:\n",
        "                yield collate_and_yield(batch)\n",
        "                batch = []\n",
        "\n",
        "print(\"Data generator `sft_data_generator` is defined.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell: Define project paths\n",
        "from pathlib import Path\n",
        "\n",
        "DATA_DIR = Path(\"data\")\n",
        "DATA_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "# Define paths for formatted data\n",
        "train_formatted_path = DATA_DIR / \"train_formatted\"\n",
        "val_formatted_path = DATA_DIR / \"val_formatted\"\n",
        "test_formatted_path = DATA_DIR / \"test_formatted\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train subset: 3000 | Validation subset: 500 | Test subset: 500\n"
          ]
        }
      ],
      "source": [
        "# Cell: Download and subset ScienceQA dataset\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Load full dataset\n",
        "full_dataset = load_dataset('derek-thomas/ScienceQA')\n",
        "\n",
        "# Create balanced subset for fine-tuning\n",
        "train_subset = full_dataset['train'].shuffle(seed=42).select(range(3000))\n",
        "val_subset = full_dataset['validation'].shuffle(seed=42).select(range(500))\n",
        "test_subset = full_dataset['test'].shuffle(seed=42).select(range(500))\n",
        "\n",
        "print(f\"Train subset: {len(train_subset)} | Validation subset: {len(val_subset)} | Test subset: {len(test_subset)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell: Define formatting function\n",
        "def format_scienceqa_for_chat(example):\n",
        "    \"\"\"Convert ScienceQA to conversational format for nanochat.\"\"\"\n",
        "    \n",
        "    # Build question with choices\n",
        "    question = example['question']\n",
        "    choices = example['choices']\n",
        "    choices_text = \"\\n\".join([f\"{chr(65+i)}. {choice}\" for i, choice in enumerate(choices)])\n",
        "    \n",
        "    full_question = f\"{question}\\n\\n{choices_text}\"\n",
        "    \n",
        "    # Build answer with explanation\n",
        "    answer_idx = example['answer']\n",
        "    correct_answer = choices[answer_idx]\n",
        "    \n",
        "    response = f\"The correct answer is {chr(65+answer_idx)}. {correct_answer}\"\n",
        "    \n",
        "    # Add explanation if available\n",
        "    if example.get('solution'):\n",
        "        response += f\"\\n\\nExplanation: {example['solution']}\"\n",
        "    \n",
        "    # Add lecture context if available\n",
        "    if example.get('lecture'):\n",
        "        response += f\"\\n\\nBackground: {example['lecture']}\"\n",
        "    \n",
        "    # Format as conversational message\n",
        "    return {\n",
        "        \"id\": example.get(\"id\"), # Keep id for tracking\n",
        "        \"messages\": [\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful science tutor for elementary through high school students. Explain concepts clearly with examples.\"},\n",
        "            {\"role\": \"user\", \"content\": full_question},\n",
        "            {\"role\": \"assistant\", \"content\": response}\n",
        "        ]\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "27ef45d3e2964683921bc5a37476ea05",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/3000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7fd2160b2b0c4efe8bb1d59692967bff",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d131d8fb65084d498417f31a4da23204",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9950a5b9d1ab487ca47d4008acb99cdc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Saving the dataset (0/1 shards):   0%|          | 0/3000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6c298436b58d4bdb850325abd7995e1b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Saving the dataset (0/1 shards):   0%|          | 0/500 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c753639bea7b4398a916b98419654b0f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Saving the dataset (0/1 shards):   0%|          | 0/500 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved formatted datasets to data\n",
            "Train: 3000 | Val: 500 | Test: 500\n"
          ]
        }
      ],
      "source": [
        "# Cell: Apply formatting and save datasets\n",
        "# Apply formatting\n",
        "train_formatted = train_subset.map(format_scienceqa_for_chat, remove_columns=train_subset.column_names)\n",
        "val_formatted = val_subset.map(format_scienceqa_for_chat, remove_columns=val_subset.column_names)\n",
        "test_formatted = test_subset.map(format_scienceqa_for_chat, remove_columns=test_subset.column_names)\n",
        "\n",
        "# Save formatted datasets to disk\n",
        "train_formatted.save_to_disk(str(train_formatted_path))\n",
        "val_formatted.save_to_disk(str(val_formatted_path))\n",
        "test_formatted.save_to_disk(str(test_formatted_path))\n",
        "\n",
        "print(f\"Saved formatted datasets to {DATA_DIR}\")\n",
        "print(f\"Train: {len(train_formatted)} | Val: {len(val_formatted)} | Test: {len(test_formatted)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded datasets for training. Train samples: 3000 | Validation samples: 500\n"
          ]
        }
      ],
      "source": [
        "# Cell: Load formatted datasets for training\n",
        "from datasets import load_from_disk\n",
        "\n",
        "# These paths were defined in a previous cell\n",
        "if not train_formatted_path.exists() or not val_formatted_path.exists():\n",
        "    raise FileNotFoundError(f\"Missing formatted datasets. Please ensure the data preparation cells have been run.\")\n",
        "\n",
        "train_dataset = load_from_disk(str(train_formatted_path))\n",
        "val_dataset = load_from_disk(str(val_formatted_path))\n",
        "\n",
        "print(f\"Loaded datasets for training. Train samples: {len(train_dataset)} | Validation samples: {len(val_dataset)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Effective batch size: 16\n",
            "Device batch size: 2\n",
            "Gradient accumulation steps: 8\n",
            "Total training iterations: 374\n"
          ]
        }
      ],
      "source": [
        "# SFT Hyperparameters (adapted from scripts/chat_sft.py)\n",
        "device_batch_size = 2 # Max to avoid OOM on a consumer GPU\n",
        "num_epochs = 2\n",
        "target_examples_per_step = 16 # Effective batch size via gradient accumulation\n",
        "unembedding_lr = 0.004\n",
        "embedding_lr = 0.2\n",
        "matrix_lr = 0.02\n",
        "weight_decay = 0.0\n",
        "init_lr_frac = 0.02\n",
        "eval_every = 100 # Steps\n",
        "eval_steps = 50\n",
        "\n",
        "# Calculate number of iterations and gradient accumulation\n",
        "grad_accum_steps = target_examples_per_step // device_batch_size\n",
        "num_iterations = (len(train_dataset) // target_examples_per_step) * num_epochs\n",
        "\n",
        "print(f\"Effective batch size: {target_examples_per_step}\")\n",
        "print(f\"Device batch size: {device_batch_size}\")\n",
        "print(f\"Gradient accumulation steps: {grad_accum_steps}\")\n",
        "print(f\"Total training iterations: {num_iterations}\")\n",
        "\n",
        "# Create data loaders (generators)\n",
        "train_loader = sft_data_generator(train_dataset, batch_size=device_batch_size)\n",
        "val_loader = sft_data_generator(val_dataset, batch_size=device_batch_size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Scaling the LR for the AdamW parameters ∝1/√(1280/768) = 0.774597\n",
            "Optimizers initialized.\n"
          ]
        }
      ],
      "source": [
        "optimizers = model.setup_optimizers(\n",
        "    unembedding_lr=unembedding_lr,\n",
        "    embedding_lr=embedding_lr,\n",
        "    matrix_lr=matrix_lr,\n",
        "    weight_decay=weight_decay,\n",
        "    )\n",
        "\n",
        "# Set the initial learning rate as a fraction of the base learning rate\n",
        "for opt in optimizers:\n",
        "    for group in opt.param_groups:\n",
        "        group[\"lr\"] = group[\"lr\"] * init_lr_frac\n",
        "        group[\"initial_lr\"] = group[\"lr\"] # save the initial learning so we can decay easily later\n",
        "\n",
        "print(\"Optimizers initialized.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting fine-tuning...\n",
            "Step 00000 | Validation loss: 1.921232\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[62], line 44\u001b[0m\n\u001b[1;32m     42\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;66;03m# for logging\u001b[39;00m\n\u001b[1;32m     43\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss \u001b[38;5;241m/\u001b[39m grad_accum_steps \u001b[38;5;66;03m# each .backward() is a grad sum => normalize loss here\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# accumulate the gradient\u001b[39;00m\n\u001b[1;32m     45\u001b[0m     num_tokens \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (train_targets \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39msum()\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# Update learning rate\u001b[39;00m\n",
            "File \u001b[0;32m~/anaconda3/envs/CSCE5720/lib/python3.11/site-packages/torch/_tensor.py:625\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    615\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    616\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    617\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    618\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    623\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    624\u001b[0m     )\n\u001b[0;32m--> 625\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    626\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/anaconda3/envs/CSCE5720/lib/python3.11/site-packages/torch/autograd/__init__.py:354\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    349\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    351\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 354\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    362\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/anaconda3/envs/CSCE5720/lib/python3.11/site-packages/torch/autograd/graph.py:841\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    839\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    840\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 841\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    842\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    843\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    844\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    845\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import time\n",
        "from contextlib import nullcontext\n",
        "from nanochat.common import autodetect_device_type\n",
        "\n",
        "device_type = \"cpu\"\n",
        "autocast_ctx = torch.amp.autocast(device_type=device_type, dtype=ptdtype) if device_type == \"cuda\" else nullcontext()\n",
        "\n",
        "# Learning rate scheduler\n",
        "def get_lr_multiplier(it):\n",
        "    lrm = 1.0 - it / num_iterations\n",
        "    return lrm\n",
        "\n",
        "print(\"Starting fine-tuning...\")\n",
        "train_iter = iter(train_loader)\n",
        "val_iter = iter(val_loader)\n",
        "val_loss = 0.0\n",
        "step_time_ema = 0.0\n",
        "\n",
        "for step in range(num_iterations):\n",
        "    t0 = time.time()\n",
        "    last_step = step == num_iterations - 1\n",
        "\n",
        "    # Validation loop\n",
        "    if last_step or step % eval_every == 0:\n",
        "        model.eval()\n",
        "        losses = []\n",
        "        for _ in range(eval_steps):\n",
        "            val_inputs, val_targets = next(val_iter)\n",
        "            with torch.no_grad(), autocast_ctx:\n",
        "                loss = model(val_inputs, val_targets)\n",
        "            losses.append(loss)\n",
        "        val_loss = torch.stack(losses).mean().item()\n",
        "        print(f\"Step {step:05d} | Validation loss: {val_loss:.6f}\")\n",
        "        model.train()\n",
        "\n",
        "    # Training loop\n",
        "    num_tokens = 0\n",
        "    for micro_step in range(grad_accum_steps):\n",
        "        train_inputs, train_targets = next(train_iter)\n",
        "        with autocast_ctx:\n",
        "            loss = model(train_inputs, train_targets)\n",
        "        train_loss = loss.detach() # for logging\n",
        "        loss = loss / grad_accum_steps # each .backward() is a grad sum => normalize loss here\n",
        "        loss.backward() # accumulate the gradient\n",
        "        num_tokens += (train_targets >= 0).sum()\n",
        "    \n",
        "    # Update learning rate\n",
        "    lrm = get_lr_multiplier(step)\n",
        "    for opt in optimizers:\n",
        "        for group in opt.param_groups:\n",
        "            group[\"lr\"] = group[\"initial_lr\"] * lrm\n",
        "\n",
        "    # Step the optimizers\n",
        "    for opt in optimizers:\n",
        "        opt.step()\n",
        "    model.zero_grad(set_to_none=True)\n",
        "\n",
        "    # Timing and logging\n",
        "    t1 = time.time()\n",
        "    dt = t1 - t0\n",
        "    step_time_ema = dt if step_time_ema == 0 else 0.9 * step_time_ema + 0.1 * dt\n",
        "    tokens_per_sec = num_tokens.item() / dt\n",
        "    \n",
        "    print(f\"Step {step:05d}/{num_iterations:05d} | Train loss: {train_loss.item():.6f} | LR mult: {lrm:.4f} | Tok/s: {tokens_per_sec:,.0f} | Step time: {step_time_ema:.3f}s\")\n",
        "\n",
        "print(\"\\nTraining complete.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "CSCE5720",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
